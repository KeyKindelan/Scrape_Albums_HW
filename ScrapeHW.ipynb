{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e261533-5f16-4528-9ede-5fa1fc60a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import requests  # Makes HTTP requests to fetch web pages from URLs\n",
    "from bs4 import BeautifulSoup  # Parses HTML content into navigable Python objects for web scraping\n",
    "import pandas as pd  # Creates and manipulates DataFrames for organizing scraped data into tables\n",
    "import time  # Adds delays between requests to avoid overwhelming the server\n",
    "from random import uniform  # Generates random time intervals to make scraping delays less predictable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f118b061-7b48-44ff-acad-1ffd682aace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create headers\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb6d1c9-3abf-4830-81a2-d0397644ea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DF from page 1 and snoozing for 36.89904923117118 seconds before next page\n",
      "Created DF from page 2 and snoozing for 41.75067165699052 seconds before next page\n",
      "Created DF from page 3 and snoozing for 42.02276311919892 seconds before next page\n",
      "Created DF from page 4 and snoozing for 41.79314933053509 seconds before next page\n",
      "Created DF from page 5 and snoozing for 41.17386190328989 seconds before next page\n",
      "Created DF from page 6 and snoozing for 30.310483080631524 seconds before next page\n",
      "Created DF from page 7 and snoozing for 43.38829212725903 seconds before next page\n",
      "Created DF from page 8 and snoozing for 34.14939987370336 seconds before next page\n",
      "Created DF from page 9 and snoozing for 33.133345127362084 seconds before next page\n",
      "Created DF from page 10 and snoozing for 30.543097555112695 seconds before next page\n",
      "Done scraping all 10 pages\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []  ## hold all dfs (dataframes) that will be created for each page\n",
    "base_url = \"https://bestsellingalbums.org/decade/2010\"  ## base URL of the site to scrape\n",
    "end_page = 10  ## total number of pages we want to scrape\n",
    "\n",
    "for url_number in range(1, end_page + 1):\n",
    "    try:  ## attempt to request the page; useful if a request fails\n",
    "        if url_number != 1:\n",
    "            ## for all pages except the first, append \"-page_number\" to the base URL\n",
    "            response = requests.get(f\"{base_url}-{url_number}\")\n",
    "        else:\n",
    "            ## first page has no suffix, so just use the base URL\n",
    "            response = requests.get(base_url)\n",
    "    except:  ## if the request fails, handle the exception gracefully\n",
    "        print(f\"Problem with {base_url}-{url_number}\")\n",
    "    finally:  ## whether successful or not, proceed to parse any response obtained\n",
    "        ## convert the page’s HTML into a BeautifulSoup object for parsing\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        ## find all album entries on the page (each album is inside a div with class 'album_card')\n",
    "        all_targets = soup.find_all(\"div\", class_=\"album_card\")\n",
    "\n",
    "        ## extract artist names from each album card\n",
    "        artists_list = [target.find(\"div\", class_=\"artist\").get_text() for target in all_targets]\n",
    "        ## extract album titles from each album card\n",
    "        albums_list = [target.find(\"div\", class_=\"album\").get_text() for target in all_targets]\n",
    "        ## extract links for more info (the href attribute of the <a> tag)\n",
    "        more_info_list = [target.find(\"a\").get(\"href\") for target in all_targets]\n",
    "        ## extract sales numbers, clean text (“Sales: ” and commas), and convert to integers\n",
    "        sales_list = [int(target.find(\"div\", class_=\"sales\").get_text().replace(\"Sales: \", \"\").replace(\",\", \"\")) \\\n",
    "                      for target in all_targets]\n",
    "        \n",
    "        ## combine the extracted lists into a dictionary and convert it into a DataFrame\n",
    "        all_dfs.append(pd.DataFrame({\"artist\": artists_list, \"album\": albums_list,\n",
    "                           \"sales\": sales_list, \"more_info\": more_info_list}))\n",
    "            \n",
    "        ## pause between page requests to avoid overwhelming the server (random delay between 30–45 seconds)\n",
    "        snoozer = uniform(30,45)\n",
    "        print(f\"Created DF from page {url_number} and snoozing for {snoozer} seconds before next page\")\n",
    "        time.sleep(snoozer)  ## actually wait the random time before continuing\n",
    "print(f\"Done scraping all {end_page} pages\")  ## confirm completion once all pages are processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea1e8b8-60cd-4d45-ac2d-981dc64883a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>sales</th>\n",
       "      <th>more_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADELE</td>\n",
       "      <td>21</td>\n",
       "      <td>30000000</td>\n",
       "      <td>https://bestsellingalbums.org/album/1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADELE</td>\n",
       "      <td>25</td>\n",
       "      <td>23000000</td>\n",
       "      <td>https://bestsellingalbums.org/album/1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MICHAEL BUBLÉ</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>15000000</td>\n",
       "      <td>https://bestsellingalbums.org/album/30524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TAYLOR SWIFT</td>\n",
       "      <td>1989</td>\n",
       "      <td>14748116</td>\n",
       "      <td>https://bestsellingalbums.org/album/45488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JUSTIN BIEBER</td>\n",
       "      <td>PURPOSE</td>\n",
       "      <td>14000000</td>\n",
       "      <td>https://bestsellingalbums.org/album/23318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>LOGIC</td>\n",
       "      <td>UNDER PRESSURE</td>\n",
       "      <td>1060000</td>\n",
       "      <td>https://bestsellingalbums.org/album/27268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>HALESTORM</td>\n",
       "      <td>THE STRANGE CASE OF</td>\n",
       "      <td>1060000</td>\n",
       "      <td>https://bestsellingalbums.org/album/17960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>ZAC BROWN BAND</td>\n",
       "      <td>UNCAGED</td>\n",
       "      <td>1055000</td>\n",
       "      <td>https://bestsellingalbums.org/album/56701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>FUTURE</td>\n",
       "      <td>FUTURE</td>\n",
       "      <td>1050371</td>\n",
       "      <td>https://bestsellingalbums.org/album/16036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>FUTURE</td>\n",
       "      <td>HNDRXX</td>\n",
       "      <td>1050000</td>\n",
       "      <td>https://bestsellingalbums.org/album/16039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist                album     sales  \\\n",
       "0             ADELE                   21  30000000   \n",
       "1             ADELE                   25  23000000   \n",
       "2     MICHAEL BUBLÉ            CHRISTMAS  15000000   \n",
       "3      TAYLOR SWIFT                 1989  14748116   \n",
       "4     JUSTIN BIEBER              PURPOSE  14000000   \n",
       "..              ...                  ...       ...   \n",
       "495           LOGIC       UNDER PRESSURE   1060000   \n",
       "496       HALESTORM  THE STRANGE CASE OF   1060000   \n",
       "497  ZAC BROWN BAND              UNCAGED   1055000   \n",
       "498          FUTURE               FUTURE   1050371   \n",
       "499          FUTURE               HNDRXX   1050000   \n",
       "\n",
       "                                     more_info  \n",
       "0     https://bestsellingalbums.org/album/1034  \n",
       "1     https://bestsellingalbums.org/album/1035  \n",
       "2    https://bestsellingalbums.org/album/30524  \n",
       "3    https://bestsellingalbums.org/album/45488  \n",
       "4    https://bestsellingalbums.org/album/23318  \n",
       "..                                         ...  \n",
       "495  https://bestsellingalbums.org/album/27268  \n",
       "496  https://bestsellingalbums.org/album/17960  \n",
       "497  https://bestsellingalbums.org/album/56701  \n",
       "498  https://bestsellingalbums.org/album/16036  \n",
       "499  https://bestsellingalbums.org/album/16039  \n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## turn into single data frame\n",
    "df = pd.concat(all_dfs, ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d008e4-1607-41ef-93ce-c2599647a0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
